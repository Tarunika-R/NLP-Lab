{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities (Hugging Face):\n",
      "El -> I-PER\n",
      "##on -> I-PER\n",
      "Mu -> I-PER\n",
      "##sk -> I-PER\n",
      "Space -> I-ORG\n",
      "##X -> I-ORG\n",
      "Te -> I-ORG\n",
      "##sla -> I-ORG\n",
      "Motors -> I-ORG\n",
      "POS Tags:\n",
      "[('Elon', 'NNP'), ('Musk', 'NNP'), ('founded', 'VBD'), ('SpaceX', 'NNP'), ('in', 'IN'), ('2002', 'CD'), ('and', 'CC'), ('Tesla', 'NNP'), ('Motors', 'NNP'), ('in', 'IN'), ('2003', 'CD'), ('.', '.')]\n",
      "HMM POS Tags:\n",
      "[('Elon', 'X'), ('Musk', 'X'), ('founded', 'X'), ('SpaceX', 'X'), ('in', 'ADP'), ('2002', 'NUM'), ('and', 'CONJ'), ('Tesla', 'ADJ'), ('Motors', 'NOUN'), ('in', 'ADP'), ('2003', 'NUM'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import hmm\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensure necessary NLTK datasets are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# 1. Named Entity Recognition (NER) using Hugging Face Transformers\n",
    "def perform_ner(sentence):\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "    ner_results = ner_pipeline(sentence)\n",
    "    print(\"Named Entities (Hugging Face):\")\n",
    "    for entity in ner_results:\n",
    "        print(f\"{entity['word']} -> {entity['entity']}\")\n",
    "\n",
    "# 2. POS tagging using NLTK\n",
    "def pos_tag_nltk(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    print(\"POS Tags:\")\n",
    "    print(pos_tags)\n",
    "\n",
    "# 3. HMM-based POS tagger\n",
    "def train_hmm_pos_tagger():\n",
    "    tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "    train_data, test_data = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n",
    "    \n",
    "    trainer = hmm.HiddenMarkovModelTrainer()\n",
    "    model = trainer.train(train_data, estimator=lambda fd, bins: LidstoneProbDist(fd, 0.1, bins))\n",
    "    \n",
    "    return model, test_data\n",
    "\n",
    "def predict_hmm(model, sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    predicted_tags = model.tag(words)\n",
    "    print(\"HMM POS Tags:\")\n",
    "    print(predicted_tags)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sentence = \"Elon Musk founded SpaceX in 2002 and Tesla Motors in 2003.\"\n",
    "    \n",
    "    # Perform NER\n",
    "    perform_ner(sentence)\n",
    "    \n",
    "    # Perform POS tagging using NLTK\n",
    "    pos_tag_nltk(sentence)\n",
    "    \n",
    "    # Train HMM POS Tagger\n",
    "    hmm_model, test_data = train_hmm_pos_tagger()\n",
    "    \n",
    "    # Predict POS tags using HMM POS Tagger\n",
    "    predict_hmm(hmm_model, sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
