{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for line 1: ['In', 'the', 'heart', 'of', 'the', 'bustling', 'city', ',', 'there', 'lies', 'a', 'small', 'park', 'that', 'serves', 'as', 'a', 'refuge', 'for', 'weary', 'souls', '.', 'The', 'park', 'is', 'adorned', 'with', 'vibrant', 'flowers', ',', 'towering', 'trees', ',', 'and', 'a', 'serene', 'pond', 'that', 'reflects', 'the', 'sky', '.', 'Every', 'morning', ',', 'joggers', 'can', 'be', 'seen', 'making', 'their', 'way', 'along', 'the', 'winding', 'paths', ',', 'while', 'children', 'play', 'joyfully', 'on', 'the', 'swings', '.', 'The', 'sound', 'of', 'laughter', 'fills', 'the', 'air', ',', 'mingling', 'with', 'the', 'chirping', 'of', 'birds', 'perched', 'on', 'branches', '.']\n",
      "Tokens for line 2: ['As', 'the', 'sun', 'rises', 'higher', ',', 'the', 'park', 'becomes', 'a', 'gathering', 'place', 'for', 'people', 'from', 'all', 'walks', 'of', 'life', '.', 'Some', 'come', 'to', 'read', 'their', 'favorite', 'books', ',', 'while', 'others', 'engage', 'in', 'deep', 'conversations', 'with', 'friends', '.', 'The', 'elderly', 'often', 'sit', 'on', 'benches', ',', 'reminiscing', 'about', 'the', 'past', 'and', 'sharing', 'stories', 'with', 'anyone', 'willing', 'to', 'listen', '.']\n",
      "Tokens for line 3: ['In', 'the', 'afternoons', ',', 'the', 'park', 'transforms', 'into', 'a', 'lively', 'scene', 'with', 'picnics', 'and', 'outdoor', 'games', '.', 'Families', 'spread', 'blankets', 'on', 'the', 'grass', ',', 'enjoying', 'homemade', 'meals', 'and', 'the', 'company', 'of', 'loved', 'ones', '.', 'The', 'aroma', 'of', 'grilled', 'food', 'wafts', 'through', 'the', 'air', ',', 'enticing', 'passersby', 'to', 'join', 'in', 'the', 'festivities', '.']\n",
      "Tokens for line 4: ['As', 'evening', 'approaches', ',', 'the', 'park', 'takes', 'on', 'a', 'different', 'ambiance', '.', 'The', 'setting', 'sun', 'casts', 'a', 'golden', 'hue', 'over', 'the', 'landscape', ',', 'and', 'the', 'sounds', 'of', 'nature', 'become', 'more', 'pronounced', '.', 'Fireflies', 'begin', 'to', 'dance', 'in', 'the', 'twilight', ',', 'creating', 'a', 'magical', 'atmosphere', 'that', 'captivates', 'all', 'who', 'visit', '.']\n",
      "Tokens for line 5: ['This', 'small', 'park', ',', 'though', 'often', 'overlooked', ',', 'holds', 'a', 'special', 'place', 'in', 'the', 'hearts', 'of', 'those', 'who', 'seek', 'solace', 'and', 'joy', 'in', 'its', 'embrace', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "with open('Document.txt', 'r') as file:\n",
    "    document = file.readlines()\n",
    "tokenized_lines = [word_tokenize(line.strip()) for line in document]\n",
    "\n",
    "for i, tokens in enumerate(tokenized_lines):\n",
    "    print(f\"Tokens for line {i+1}: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words for line 1: ['in', 'the', 'heart', 'of', 'the', 'bustl', 'citi', ',', 'there', 'lie', 'a', 'small', 'park', 'that', 'serv', 'as', 'a', 'refug', 'for', 'weari', 'soul', '.', 'the', 'park', 'is', 'adorn', 'with', 'vibrant', 'flower', ',', 'tower', 'tree', ',', 'and', 'a', 'seren', 'pond', 'that', 'reflect', 'the', 'sky', '.', 'everi', 'morn', ',', 'jogger', 'can', 'be', 'seen', 'make', 'their', 'way', 'along', 'the', 'wind', 'path', ',', 'while', 'children', 'play', 'joy', 'on', 'the', 'swing', '.', 'the', 'sound', 'of', 'laughter', 'fill', 'the', 'air', ',', 'mingl', 'with', 'the', 'chirp', 'of', 'bird', 'perch', 'on', 'branch', '.']\n",
      "Stemmed words for line 2: ['as', 'the', 'sun', 'rise', 'higher', ',', 'the', 'park', 'becom', 'a', 'gather', 'place', 'for', 'peopl', 'from', 'all', 'walk', 'of', 'life', '.', 'some', 'come', 'to', 'read', 'their', 'favorit', 'book', ',', 'while', 'other', 'engag', 'in', 'deep', 'convers', 'with', 'friend', '.', 'the', 'elderli', 'often', 'sit', 'on', 'bench', ',', 'reminisc', 'about', 'the', 'past', 'and', 'share', 'stori', 'with', 'anyon', 'will', 'to', 'listen', '.']\n",
      "Stemmed words for line 3: ['in', 'the', 'afternoon', ',', 'the', 'park', 'transform', 'into', 'a', 'live', 'scene', 'with', 'picnic', 'and', 'outdoor', 'game', '.', 'famili', 'spread', 'blanket', 'on', 'the', 'grass', ',', 'enjoy', 'homemad', 'meal', 'and', 'the', 'compani', 'of', 'love', 'one', '.', 'the', 'aroma', 'of', 'grill', 'food', 'waft', 'through', 'the', 'air', ',', 'entic', 'passersbi', 'to', 'join', 'in', 'the', 'festiv', '.']\n",
      "Stemmed words for line 4: ['as', 'even', 'approach', ',', 'the', 'park', 'take', 'on', 'a', 'differ', 'ambianc', '.', 'the', 'set', 'sun', 'cast', 'a', 'golden', 'hue', 'over', 'the', 'landscap', ',', 'and', 'the', 'sound', 'of', 'natur', 'becom', 'more', 'pronounc', '.', 'firefli', 'begin', 'to', 'danc', 'in', 'the', 'twilight', ',', 'creat', 'a', 'magic', 'atmospher', 'that', 'captiv', 'all', 'who', 'visit', '.']\n",
      "Stemmed words for line 5: ['thi', 'small', 'park', ',', 'though', 'often', 'overlook', ',', 'hold', 'a', 'special', 'place', 'in', 'the', 'heart', 'of', 'those', 'who', 'seek', 'solac', 'and', 'joy', 'in', 'it', 'embrac', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_lines = [[stemmer.stem(token) for token in tokens] for tokens in tokenized_lines]\n",
    "\n",
    "# Output the stemmed words\n",
    "for i, stemmed in enumerate(stemmed_lines):\n",
    "    print(f\"Stemmed words for line {i + 1}: {stemmed}\")\n",
    "\n",
    "# Stemmed words to a new text file\n",
    "with open('stemmed_words.txt', 'w') as output_file:\n",
    "    for i, stemmed in enumerate(stemmed_lines):\n",
    "        output_file.write(f\"Stemmed words for line {i + 1}: {stemmed}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('For', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('containing', 'VBG'), ('words', 'NNS'), ('like', 'IN'), ('visit', 'NN'), (',', ','), ('visitor', 'NN'), (',', ','), ('visiting', 'VBG'), (',', ','), ('visited', 'VBD')]\n",
      "Lemmatized words:\n",
      "['For', 'a', 'sentence', 'contain', 'word', 'like', 'visit', ',', 'visitor', ',', 'visit', ',', 'visit']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "sample_sentence = \"For a sentence containing  words like visit, visitor, visiting, visited\"\n",
    "tokens = word_tokenize(sample_sentence)\n",
    "\n",
    "token_tags = pos_tag(tokens)\n",
    "print(token_tags)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  \n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  \n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  \n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  \n",
    "    else:\n",
    "        return None  \n",
    "    \n",
    "lemmatized_words = []\n",
    "for token, tag in token_tags:\n",
    "    wordnet_pos = get_wordnet_pos(tag) or 'n' # Default to noun if no tag found\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(token, pos=wordnet_pos))\n",
    "\n",
    "print(\"Lemmatized words:\")\n",
    "print(lemmatized_words)\n",
    "\n",
    "with open('lemmatized_words.txt', 'w') as output_file:\n",
    "    output_file.write(\"Lemmatized words:\\n\")\n",
    "    output_file.write(', '.join(lemmatized_words) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"wasn't\", 'here', 'can', 'from', 'you', \"you've\", 'those', 'their', 'being', 'not', 'mightn', 'why', 'about', 'are', 'him', \"she's\", 'there', 'ma', 'so', 'they', 'now', 'm', 'more', 'up', 'was', \"you'll\", 'for', 'only', 'were', 'a', 'against', 'out', 'itself', 'the', 'few', 'just', 'by', 'who', 't', \"wouldn't\", 'above', 'any', 'such', 've', 'won', 'because', 'it', \"isn't\", 'both', 'what', 'but', 'before', 'wasn', 'same', 'over', 'hasn', 'yourselves', 'shan', 'should', 'don', \"weren't\", 'while', 'wouldn', 'this', 'd', 'have', 'aren', 'himself', 'o', \"shouldn't\", \"you'd\", 'or', 'down', 'doesn', \"mightn't\", 'am', 'needn', 'further', 'hadn', 'themselves', 'where', 're', \"shan't\", 'mustn', 'some', \"couldn't\", 'his', 'its', 'yourself', \"should've\", 'when', 'that', 'each', 'off', 'how', 'until', 'your', \"it's\", 'does', 'between', 'ourselves', 'has', \"that'll\", 'ain', 'didn', 'with', 's', 'through', 'll', 'isn', 'whom', 'under', \"doesn't\", 'no', 'than', 'our', 'as', 'we', 'myself', 'of', 'own', \"won't\", 'is', 'doing', \"didn't\", 'having', 'if', 'them', 'after', \"don't\", 'he', 'be', 'to', \"needn't\", 'into', 'y', 'my', 'these', 'hers', 'again', 'most', 'all', 'very', 'do', \"you're\", 'once', 'weren', \"hasn't\", 'during', \"hadn't\", 'i', 'at', 'other', 'ours', 'below', \"haven't\", 'too', 'her', 'which', 'theirs', \"mustn't\", 'yours', 'had', 'herself', 'then', 'will', 'did', 'and', 'been', 'couldn', 'an', 'me', 'nor', 'in', 'shouldn', 'haven', 'she', \"aren't\", 'on'}\n",
      "Filtered words for line 1: ['heart', 'bustling', 'city', ',', 'lies', 'small', 'park', 'serves', 'refuge', 'weary', 'souls', '.', 'park', 'adorned', 'vibrant', 'flowers', ',', 'towering', 'trees', ',', 'serene', 'pond', 'reflects', 'sky', '.', 'Every', 'morning', ',', 'joggers', 'seen', 'making', 'way', 'along', 'winding', 'paths', ',', 'children', 'play', 'joyfully', 'swings', '.', 'sound', 'laughter', 'fills', 'air', ',', 'mingling', 'chirping', 'birds', 'perched', 'branches', '.']\n",
      "Filtered words for line 2: ['sun', 'rises', 'higher', ',', 'park', 'becomes', 'gathering', 'place', 'people', 'walks', 'life', '.', 'come', 'read', 'favorite', 'books', ',', 'others', 'engage', 'deep', 'conversations', 'friends', '.', 'elderly', 'often', 'sit', 'benches', ',', 'reminiscing', 'past', 'sharing', 'stories', 'anyone', 'willing', 'listen', '.']\n",
      "Filtered words for line 3: ['afternoons', ',', 'park', 'transforms', 'lively', 'scene', 'picnics', 'outdoor', 'games', '.', 'Families', 'spread', 'blankets', 'grass', ',', 'enjoying', 'homemade', 'meals', 'company', 'loved', 'ones', '.', 'aroma', 'grilled', 'food', 'wafts', 'air', ',', 'enticing', 'passersby', 'join', 'festivities', '.']\n",
      "Filtered words for line 4: ['evening', 'approaches', ',', 'park', 'takes', 'different', 'ambiance', '.', 'setting', 'sun', 'casts', 'golden', 'hue', 'landscape', ',', 'sounds', 'nature', 'become', 'pronounced', '.', 'Fireflies', 'begin', 'dance', 'twilight', ',', 'creating', 'magical', 'atmosphere', 'captivates', 'visit', '.']\n",
      "Filtered words for line 5: ['small', 'park', ',', 'though', 'often', 'overlooked', ',', 'holds', 'special', 'place', 'hearts', 'seek', 'solace', 'joy', 'embrace', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "tokenized_lines = [word_tokenize(line.strip()) for line in document]\n",
    "\n",
    "filtered_lines = []\n",
    "for tokens in tokenized_lines:\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_lines.append(filtered_tokens)\n",
    "\n",
    "for i, filtered in enumerate(filtered_lines):\n",
    "    print(f\"Filtered words for line {i + 1}: {filtered}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
