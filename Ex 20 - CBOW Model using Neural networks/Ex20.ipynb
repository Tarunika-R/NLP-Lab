{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary (word to index mapping):\n",
      "{'the': 0, 'brown': 1, 'fox': 2, 'dog': 3, 'jumps': 4, 'quick': 5, 'lazy': 6, 'over': 7}\n",
      "\n",
      "Training Data (Context -> Target):\n",
      "['the', 'quick', 'fox', 'jumps'] -> brown\n",
      "['quick', 'brown', 'jumps', 'over'] -> fox\n",
      "['brown', 'fox', 'over', 'the'] -> jumps\n",
      "['fox', 'jumps', 'the', 'lazy'] -> over\n",
      "['jumps', 'over', 'lazy', 'dog'] -> the\n",
      "\n",
      "X_train (Context indices):\n",
      "tensor([[0, 5, 2, 4],\n",
      "        [5, 1, 4, 7],\n",
      "        [1, 2, 7, 0],\n",
      "        [2, 4, 0, 6],\n",
      "        [4, 7, 6, 3]])\n",
      "\n",
      "y_train (Target indices):\n",
      "tensor([1, 2, 4, 7, 0])\n",
      "\n",
      "CBOW Model Architecture:\n",
      " CBOW(\n",
      "  (embeddings): Embedding(8, 10)\n",
      "  (linear): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (activation): LogSoftmax(dim=1)\n",
      ")\n",
      "Epoch [10/100], Loss: 10.0330\n",
      "Epoch [20/100], Loss: 9.0920\n",
      "Epoch [30/100], Loss: 8.3306\n",
      "Epoch [40/100], Loss: 7.6909\n",
      "Epoch [50/100], Loss: 7.1388\n",
      "Epoch [60/100], Loss: 6.6528\n",
      "Epoch [70/100], Loss: 6.2189\n",
      "Epoch [80/100], Loss: 5.8275\n",
      "Epoch [90/100], Loss: 5.4714\n",
      "Epoch [100/100], Loss: 5.1455\n",
      "\n",
      "Word Embeddings:\n",
      "the: [-1.9781982   0.09667669 -1.2961369  -1.3434584   1.3049432   0.02252365\n",
      " -0.02619437 -0.22082245  1.8177553   1.7586116 ]\n",
      "brown: [-1.0463533  -0.6171687  -2.1918137  -1.7017704   1.0344284   1.2306998\n",
      " -0.5476231   1.6393919  -1.8741231  -0.34731954]\n",
      "fox: [-0.07670655  1.9490625  -1.82765     1.255422   -1.428963    1.7895913\n",
      " -0.37910545 -0.56866527  0.3110199   0.7427568 ]\n",
      "dog: [ 0.80813634 -0.42974865 -2.7355382  -1.1422956  -0.59524375  0.782214\n",
      "  0.7872101   1.0568861   0.17948204  1.3681946 ]\n",
      "jumps: [ 1.5002719   1.0457785  -1.0668582  -0.6576753   0.41801992  0.06031908\n",
      "  0.2665927   0.8540837   1.7058359  -1.1949044 ]\n",
      "quick: [ 0.05916593  0.1344808   0.01792606 -1.9881442  -1.2302965   1.066199\n",
      "  1.2105767  -0.99005896  1.6064845   1.4107039 ]\n",
      "lazy: [ 1.6039876  -0.92687076  0.44827858 -0.281158    0.20038672 -0.71023077\n",
      "  0.10464491 -0.84810126 -1.3341942  -0.03253325]\n",
      "over: [ 0.6279872   0.8158169  -0.8630341   1.030094   -1.7431083   1.331574\n",
      " -0.8866408   1.178408   -0.26088136 -0.07740817]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Preprocess the text\n",
    "words = corpus.lower().split()\n",
    "vocab = set(words)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Print Vocabulary\n",
    "print(\"\\nVocabulary (word to index mapping):\")\n",
    "print(word_to_idx)\n",
    "\n",
    "# Define CBOW parameters\n",
    "context_size = 2  # Number of context words on both sides\n",
    "embedding_dim = 10\n",
    "\n",
    "# Prepare training data\n",
    "data = []\n",
    "for i in range(context_size, len(words) - context_size):\n",
    "    context = [words[j] for j in range(i - context_size, i)] + [words[j] for j in range(i + 1, i + context_size + 1)]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "# Print Training Data\n",
    "print(\"\\nTraining Data (Context -> Target):\")\n",
    "for context, target in data:\n",
    "    print(f\"{context} -> {target}\")\n",
    "\n",
    "# Convert words to tensor indices\n",
    "X_train = []\n",
    "y_train = []\n",
    "for context, target in data:\n",
    "    X_train.append([word_to_idx[w] for w in context])\n",
    "    y_train.append(word_to_idx[target])\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Print Input-Output Pairs (Tensors)\n",
    "print(\"\\nX_train (Context indices):\")\n",
    "print(X_train)\n",
    "print(\"\\ny_train (Target indices):\")\n",
    "print(y_train)\n",
    "\n",
    "# Define the CBOW Model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.activation = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embeds = self.embeddings(context).mean(dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return self.activation(out)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Print CBOW Model Summary\n",
    "print(\"\\nCBOW Model Architecture:\\n\", model)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X_train)):\n",
    "        context = X_train[i].unsqueeze(0)  # Add batch dimension\n",
    "        target = y_train[i].unsqueeze(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(context)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "# Testing: Get word embeddings\n",
    "def get_word_embedding(word):\n",
    "    word_idx = torch.tensor([word_to_idx[word]], dtype=torch.long)\n",
    "    return model.embeddings(word_idx).detach().numpy()\n",
    "\n",
    "# Print Word Embeddings for Each Word in Vocabulary\n",
    "print(\"\\nWord Embeddings:\")\n",
    "for word in vocab:\n",
    "    embedding = get_word_embedding(word)\n",
    "    print(f'{word}: {embedding.flatten()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
