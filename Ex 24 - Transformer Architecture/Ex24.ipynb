{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a168b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 3.5413\n",
      "Epoch 2 Loss: 1.8870\n",
      "Epoch 3 Loss: 1.0239\n",
      "Epoch 4 Loss: 0.6197\n",
      "Epoch 5 Loss: 0.4051\n",
      "Epoch 6 Loss: 0.2640\n",
      "Epoch 7 Loss: 0.1651\n",
      "Epoch 8 Loss: 0.0995\n",
      "Epoch 9 Loss: 0.0570\n",
      "Epoch 10 Loss: 0.0451\n",
      "Translation: thank you\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Sample toy dataset\n",
    "data = [\n",
    "    (\"bonjour\", \"hello\"),\n",
    "    (\"je suis Ã©tudiant\", \"i am a student\"),\n",
    "    (\"j'aime le football\", \"i like football\"),\n",
    "    (\"il fait beau\", \"it is sunny\"),\n",
    "    (\"merci\", \"thank you\")\n",
    "]\n",
    "\n",
    "# Tokenizer + Vocabulary\n",
    "class Vocab:\n",
    "    def __init__(self, sentences):\n",
    "        tokens = set()\n",
    "        for sentence in sentences:\n",
    "            tokens.update(sentence.strip().split())\n",
    "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
    "        for i, token in enumerate(tokens, 4):\n",
    "            self.word2idx[token] = i\n",
    "            self.idx2word[i] = token\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.word2idx.get(word, 3) for word in sentence.strip().split()] + [2]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        words = [self.idx2word[token] for token in tokens if token not in [0, 1, 2]]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Create vocabularies\n",
    "french_vocab = Vocab([fr for fr, en in data])\n",
    "english_vocab = Vocab([en for fr, en in data])\n",
    "\n",
    "# Custom Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fr, en = self.data[idx]\n",
    "        return torch.tensor(french_vocab.encode(fr)), torch.tensor([1] + english_vocab.encode(en))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    fr_sentences, en_sentences = zip(*batch)\n",
    "    fr_pad = nn.utils.rnn.pad_sequence(fr_sentences, padding_value=0)\n",
    "    en_pad = nn.utils.rnn.pad_sequence(en_sentences, padding_value=0)\n",
    "    return fr_pad, en_pad\n",
    "\n",
    "dataset = TranslationDataset(data)\n",
    "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=128, nhead=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = nn.Parameter(self._generate_positional_encoding(d_model, 100))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def _generate_positional_encoding(self, d_model, max_len):\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        i = torch.arange(0, d_model, 2).float()\n",
    "        angle_rates = 1 / (10000 ** (i / d_model))\n",
    "        angle_rads = pos * angle_rates\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(angle_rads)\n",
    "        pe[:, 1::2] = torch.cos(angle_rads)\n",
    "        return pe.unsqueeze(1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.src_embed(src) + self.pos_encoder[:src.size(0)]\n",
    "        tgt = self.tgt_embed(tgt) + self.pos_encoder[:tgt.size(0)]\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(len(french_vocab), len(english_vocab)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "def train(model, loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for src, tgt in loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:-1, :]\n",
    "            tgt_output = tgt[1:, :]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n",
    "\n",
    "train(model, loader)\n",
    "\n",
    "# Inference\n",
    "def translate_sentence(model, sentence):\n",
    "    model.eval()\n",
    "    tokens = french_vocab.encode(sentence)\n",
    "    src = torch.tensor(tokens).unsqueeze(1).to(device)\n",
    "    src = model.src_embed(src) + model.pos_encoder[:src.size(0)]\n",
    "    memory = model.transformer.encoder(src)\n",
    "\n",
    "    ys = torch.ones(1, 1).fill_(1).type(torch.long).to(device)  # <sos>\n",
    "    for i in range(20):\n",
    "        tgt_emb = model.tgt_embed(ys) + model.pos_encoder[:ys.size(0)]\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(ys.size(0)).to(device)\n",
    "        out = model.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        out = model.fc_out(out)\n",
    "        next_token = out.argmax(dim=-1)[-1, 0].item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(ys).fill_(next_token)], dim=0)\n",
    "        if next_token == 2:\n",
    "            break\n",
    "    return english_vocab.decode(ys.squeeze().tolist())\n",
    "\n",
    "# Example usage\n",
    "print(\"Translation:\", translate_sentence(model, \"salut\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
