{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in Brown Corpus:\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist, word_tokenize, sent_tokenize\n",
    "\n",
    "# Ensure the Brown Corpus is downloaded\n",
    "nltk.download('brown')\n",
    "\n",
    "# Explore the available categories (sections) in the Brown Corpus\n",
    "categories = brown.categories()\n",
    "print(\"Categories in Brown Corpus:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents Categorized by Sections:\n",
      "adventure: 29 documents\n",
      "belles_lettres: 75 documents\n",
      "editorial: 27 documents\n",
      "fiction: 29 documents\n",
      "government: 30 documents\n",
      "hobbies: 36 documents\n",
      "humor: 9 documents\n",
      "learned: 80 documents\n",
      "lore: 48 documents\n",
      "mystery: 24 documents\n",
      "news: 44 documents\n",
      "religion: 17 documents\n",
      "reviews: 17 documents\n",
      "romance: 29 documents\n",
      "science_fiction: 6 documents\n",
      "\n",
      "Analysis of 'humor' Section:\n",
      "Number of words: 21695\n",
      "Number of sentences: 1053\n",
      "Modal verb frequencies: {'might': 8, 'would': 56, 'can': 17, 'could': 33, 'may': 8, 'should': 7, 'will': 13, 'must': 9, 'shall': 2}\n",
      "'Wh' word frequencies: {'why': 13, 'who': 49, 'which': 62, 'when': 62, 'what': 46, 'where': 16, 'whose': 8, 'whom': 4}\n"
     ]
    }
   ],
   "source": [
    "# Categorize documents for each section in the Brown Corpus\n",
    "category_documents = {category: brown.fileids(category) for category in categories}\n",
    "print(\"\\nDocuments Categorized by Sections:\")\n",
    "for category, docs in category_documents.items():\n",
    "    print(f\"{category}: {len(docs)} documents\")\n",
    "\n",
    "# Choose a section (e.g., 'news') for further analysis\n",
    "chosen_section = 'humor'\n",
    "\n",
    "# Words and sentences in the chosen section\n",
    "section_words = brown.words(categories=chosen_section)\n",
    "section_sentences = brown.sents(categories=chosen_section)\n",
    "\n",
    "# Count the number of words and sentences\n",
    "num_words = len(section_words)\n",
    "num_sentences = len(section_sentences)\n",
    "\n",
    "# Count modal verbs in the chosen section\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would']\n",
    "modal_freq = FreqDist(word.lower() for word in section_words if word.lower() in modals)\n",
    "\n",
    "# Count 'wh' words in the chosen section\n",
    "wh_words = ['what', 'why', 'whom', 'who', 'which', 'when', 'where', 'whose']\n",
    "wh_word_freq = FreqDist(word.lower() for word in section_words if word.lower() in wh_words)\n",
    "\n",
    "# Display details of the chosen section\n",
    "print(f\"\\nAnalysis of '{chosen_section}' Section:\")\n",
    "print(f\"Number of words: {num_words}\")\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "print(f\"Modal verb frequencies: {dict(modal_freq)}\")\n",
    "print(f\"'Wh' word frequencies: {dict(wh_word_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content of the file 'cr01' in 'humor' section (first 100 words):\n",
      "It was among these that Hinkle identified a photograph of Barco ! ! For it seems that Barco , fancying himself a ladies' man ( and why not , after seven marriages ? ? ) , had listed himself for Mormon Beard roles at the instigation of his fourth murder victim who had said : `` With your beard , dear , you ought to be in movies '' ! ! Mills secured Barco's photograph from the gentleman in charge , rushed to the Hollywood police station to report the theft , and less than five minutes later , detectives\n",
      "\n",
      "Additional Details:\n",
      "Total categories: 15\n",
      "First 5 categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government']\n",
      "Sample documents in 'humor' section: ['cr01', 'cr02', 'cr03', 'cr04', 'cr05']\n"
     ]
    }
   ],
   "source": [
    "# Explore additional corpus functionalities\n",
    "# Displaying a sample file's content\n",
    "sample_file = category_documents[chosen_section][0]\n",
    "sample_content = brown.words(sample_file)\n",
    "print(f\"\\nContent of the file '{sample_file}' in '{chosen_section}' section (first 100 words):\")\n",
    "print(\" \".join(sample_content[:100]))\n",
    "\n",
    "# Displaying categories, genres, and more\n",
    "print(\"\\nAdditional Details:\")\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "print(f\"First 5 categories: {categories[:5]}\")\n",
    "print(f\"Sample documents in '{chosen_section}' section: {category_documents[chosen_section][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring Brown Corpus Functionality:\n",
      "\n",
      "fileids():\n",
      "Total files: 500\n",
      "Sample file IDs: ['ca01', 'ca02', 'ca03', 'ca04', 'ca05']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Ensure the Brown Corpus is downloaded\n",
    "nltk.download('brown')\n",
    "\n",
    "# Example and Description\n",
    "print(\"\\nExploring Brown Corpus Functionality:\\n\")\n",
    "\n",
    "# fileids()\n",
    "print(\"fileids():\")\n",
    "file_ids = brown.fileids()\n",
    "print(f\"Total files: {len(file_ids)}\")\n",
    "print(f\"Sample file IDs: {file_ids[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileids([categories]):\n",
      "Files in 'humor' category: ['cr01', 'cr02', 'cr03', 'cr04', 'cr05']\n",
      "\n",
      "categories():\n",
      "Total categories: 15\n",
      "Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n",
      "categories([fileids]):\n",
      "Category for file 'ca01': ['news']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fileids([categories])\n",
    "print(\"fileids([categories]):\")\n",
    "print(f\"Files in 'humor' category: {brown.fileids(categories=['humor'])[:5]}\\n\")\n",
    "\n",
    "# categories()\n",
    "print(\"categories():\")\n",
    "categories = brown.categories()\n",
    "print(f\"Total categories: {len(categories)}\")\n",
    "print(f\"Categories: {categories}\\n\")\n",
    "\n",
    "# categories([fileids])\n",
    "print(\"categories([fileids]):\")\n",
    "sample_file = file_ids[0]\n",
    "print(f\"Category for file '{sample_file}': {brown.categories(fileids=[sample_file])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw():\n",
      "Total raw content length: 9964284 characters\n",
      "\n",
      "raw(fileids=[f1,f2,f3]):\n",
      "Raw content of 'ca01': \n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' tha...\n",
      "\n",
      "raw(categories=[c1,c2]):\n",
      "Raw content of 'humor' category: It/pps was/bedz among/in these/dts that/cs Hinkle/np identified/vbd a/at photograph/nn of/in Barco/np !/. !/.\n",
      "For/cs it/pps seems/vbz that/cs Barco/np ,/, fancying/vbg himself/ppl a/at ladies'/nns$ ma...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw()\n",
    "print(\"raw():\")\n",
    "print(f\"Total raw content length: {len(brown.raw())} characters\\n\")\n",
    "\n",
    "# raw(fileids=[f1,f2,f3])\n",
    "print(\"raw(fileids=[f1,f2,f3]):\")\n",
    "print(f\"Raw content of '{sample_file}': {brown.raw(fileids=[sample_file])[:200]}...\\n\")\n",
    "\n",
    "# raw(categories=[c1,c2])\n",
    "print(\"raw(categories=[c1,c2]):\")\n",
    "print(f\"Raw content of 'humor' category: {brown.raw(categories=['humor'])[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words():\n",
      "Total words in corpus: 1161192\n",
      "First 10 words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "\n",
      "words(fileids=[f1,f2,f3]):\n",
      "Words in 'ca01': ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "\n",
      "words(categories=[c1,c2]):\n",
      "Words in 'humor' category: ['It', 'was', 'among', 'these', 'that', 'Hinkle', 'identified', 'a', 'photograph', 'of']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# words()\n",
    "print(\"words():\")\n",
    "print(f\"Total words in corpus: {len(brown.words())}\")\n",
    "print(f\"First 10 words: {brown.words()[:10]}\\n\")\n",
    "\n",
    "# words(fileids=[f1,f2,f3])\n",
    "print(\"words(fileids=[f1,f2,f3]):\")\n",
    "print(f\"Words in '{sample_file}': {brown.words(fileids=[sample_file])[:10]}\\n\")\n",
    "\n",
    "# words(categories=[c1,c2])\n",
    "print(\"words(categories=[c1,c2]):\")\n",
    "print(f\"Words in 'humor' category: {brown.words(categories=['humor'])[:10]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents():\n",
      "Total sentences in corpus: 57340\n",
      "First sentence: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "sents(fileids=[f1,f2,f3]):\n",
      "First sentence in 'ca01': The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "sents(categories=[c1,c2]):\n",
      "First sentence in 'humor' category: It was among these that Hinkle identified a photograph of Barco ! !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sents()\n",
    "print(\"sents():\")\n",
    "print(f\"Total sentences in corpus: {len(brown.sents())}\")\n",
    "print(f\"First sentence: {' '.join(brown.sents()[0])}\\n\")\n",
    "\n",
    "# sents(fileids=[f1,f2,f3])\n",
    "print(\"sents(fileids=[f1,f2,f3]):\")\n",
    "print(f\"First sentence in '{sample_file}': {' '.join(brown.sents(fileids=[sample_file])[0])}\\n\")\n",
    "\n",
    "# sents(categories=[c1,c2])\n",
    "print(\"sents(categories=[c1,c2]):\")\n",
    "print(f\"First sentence in 'humor' category: {' '.join(brown.sents(categories=['humor'])[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abspath(fileid):\n",
      "Absolute path of 'ca01': C:\\Users\\Hema\\AppData\\Roaming\\nltk_data\\corpora\\brown\\ca01\n",
      "\n",
      "encoding(fileid):\n",
      "Encoding of 'ca01': ascii\n",
      "\n",
      "open(fileid):\n",
      "First 200 characters from 'ca01': \n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' tha...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# abspath(fileid)\n",
    "print(\"abspath(fileid):\")\n",
    "print(f\"Absolute path of '{sample_file}': {brown.abspath(sample_file)}\\n\")\n",
    "\n",
    "# encoding(fileid)\n",
    "print(\"encoding(fileid):\")\n",
    "print(f\"Encoding of '{sample_file}': {brown.encoding(sample_file)}\\n\")\n",
    "\n",
    "# open(fileid)\n",
    "print(\"open(fileid):\")\n",
    "with brown.open(sample_file) as f:\n",
    "    print(f\"First 200 characters from '{sample_file}': {f.read(200)}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root():\n",
      "Root path of Brown Corpus: C:\\Users\\Hema\\AppData\\Roaming\\nltk_data\\corpora\\brown\n",
      "\n",
      "readme():\n",
      "Contents of README file:\n",
      "BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# root()\n",
    "print(\"root():\")\n",
    "print(f\"Root path of Brown Corpus: {brown.root}\\n\")\n",
    "\n",
    "# readme()\n",
    "print(\"readme():\")\n",
    "print(\"Contents of README file:\")\n",
    "print(brown.readme()[:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
