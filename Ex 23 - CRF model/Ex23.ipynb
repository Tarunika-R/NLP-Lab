{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31648559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8912132193fc43548732746f90d4f309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b430deebc7040eaacbaa310ddc6c9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2f2d1dbb6a42eeab75079432cceacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12908f3299364ff2a2cb02886d67c06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.843     0.814     0.828      1668\n",
      "      B-MISC      0.823     0.756     0.788       702\n",
      "       B-ORG      0.765     0.729     0.746      1661\n",
      "       B-PER      0.829     0.848     0.839      1617\n",
      "       I-LOC      0.726     0.630     0.675       257\n",
      "      I-MISC      0.632     0.667     0.649       216\n",
      "       I-ORG      0.710     0.726     0.718       835\n",
      "       I-PER      0.867     0.952     0.908      1156\n",
      "           O      0.988     0.989     0.989     38323\n",
      "\n",
      "    accuracy                          0.956     46435\n",
      "   macro avg      0.798     0.790     0.793     46435\n",
      "weighted avg      0.956     0.956     0.956     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from datasets import load_dataset\n",
    "from sklearn_crfsuite import metrics\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# ✅ Load CoNLL-2003 dataset with trust_remote_code enabled\n",
    "conll_data = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "# ✅ Extract POS/NER tag names\n",
    "ner_label_list = conll_data[\"train\"].features[\"ner_tags\"].feature.names\n",
    "pos_label_list = conll_data[\"train\"].features[\"pos_tags\"].feature.names\n",
    "\n",
    "# ✅ Convert dataset into list of sentences\n",
    "def prepare_data(dataset):\n",
    "    sentences = []\n",
    "    for words, pos_tags, ner_tags in zip(dataset['tokens'], dataset['pos_tags'], dataset['ner_tags']):\n",
    "        sentence = list(zip(words, pos_tags, ner_tags))\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "train_sentences = prepare_data(conll_data['train'])\n",
    "test_sentences = prepare_data(conll_data['test'])\n",
    "\n",
    "# ✅ Word-level + contextual feature extractor\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = pos_label_list[sent[i][1]]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],         # suffix\n",
    "        'word[-2:]': word[-2:],        # suffix\n",
    "        'word[:2]': word[:2],           # prefix\n",
    "        'word[:3]': word[:3],           # prefix\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag\n",
    "    }\n",
    "\n",
    "    # Previous word context\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = pos_label_list[sent[i - 1][1]]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of Sentence\n",
    "\n",
    "    # Next word context\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = pos_label_list[sent[i + 1][1]]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of Sentence\n",
    "\n",
    "    return features\n",
    "\n",
    "# ✅ Apply feature extraction and label formatting\n",
    "def extract_features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def get_labels(sent):\n",
    "    return [ner_label_list[label] for (_, _, label) in sent]\n",
    "\n",
    "X_train = [extract_features(s) for s in train_sentences]\n",
    "y_train = [get_labels(s) for s in train_sentences]\n",
    "\n",
    "X_test = [extract_features(s) for s in test_sentences]\n",
    "y_test = [get_labels(s) for s in test_sentences]\n",
    "\n",
    "# ✅ Train CRF model\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Predict and evaluate\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
